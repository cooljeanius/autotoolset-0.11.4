This is tutorial.info, produced by makeinfo version 4.3 from
tutorial.texi.

INFO-DIR-SECTION Miscellaneous
START-INFO-DIR-ENTRY
* Autotoolset Tutorial: (tutorial).        Autotools tutorial.
END-INFO-DIR-ENTRY


File: tutorial.info,  Node: Building libraries,  Prev: Problems with Makefiles and workarounds,  Up: Compiling with Makefiles

Building libraries
==================

   There's one last thing that we need to mention before moving on, and
that's "libraries". As you recall, to put together an executable, we
make a whole bunch of `.o' files and then put them all together. It just
so happens in many cases that a set of `.o' files together forms a
cohesive unit that can be reused in many applications, and you'd like
to use them in other programs. To make things simpler, what you do is
put the `.o' files together and make a "library".

   A library is usually composed of many `.c' files and hopefully only
one or at most two `.h' files. It's a good practice to minimize the use
of header files and put all your gunk in one header file, because this
way the user of your library won't have to be typing an endless stream
of `#include' directives for _every_ `.c' file he writes that depends
on the library. Be considerate. The user might be you! Header files
fall under two categories: "public" and "private". The public header
files must be installed at `/prefix/include' whereas the private ones
are only meant to be used internally. The public header files export
documented library features to the user. The private header files export
undocumented library features that are to be used only by the developer
of the library and only for the purpose of developing the library.

   Suppose that we have a library called `barf' that's made of the
following files:

     `barf.h', `barf1.c', `barf2.c', `barf3.c'

In real life, the names should be more meaningful than that, but we're
being general here. To build it, you first make the `.o' files:
     % gcc -c barf1.c
     % gcc -c barf2.c
     % gcc -c barf3.c

and then you do this magic:
     % rm -f libbarf.a
     % ar cru libbarf.a barf1.o barf2.o barf3.o

This will create a file `libbarf.a' from the object files `barf1.o',
`barf2.o', `barf3.p'.  On most Unix systems, the library won't work
unless it's "blessed" by a program called `ranlib':
     % ranlib libbarf.a

On other Unix systems, you might find that `ranlib' doesn't even exist
because it's not needed.

   The reason for this is historical. Originally `ar' was meant to be
used merely for packaging files together. The more well known program
`tar' is a descendent of `ar' that was designed to handle making such
archives on a tape device. Now that tape devices are more or less
obsolete, `tar' is playing the role that was originally meant for `ar'.
As for `ar', way back, some people thought to use it to package `*.o'
files. However the linker wanted a symbol table to be passed along with
the archive for the convenience of the people writing the code for the
linker. Perhaps also for efficiency. So the `ranlib' program was
written to generate that table and add it to the `*.a' file.  Then some
Unix vendors thought that if they incorporated `ranlib' to `ar' then
users wouldn't have to worry about forgetting to call `ranlib'. So they
provided `ranlib' but it did nothing. Some of the more evil ones
dropped it all-together breaking many people's makefiles that tried to
run `ranlib'. In the next chapter we will show you that Autoconf and
Automake will automatically determine for you how to deal with `ranlib'
in a portable manner.

   Anyway, once you have a library, you put the header file `barf.h'
under `/usr/local/include' and the `libbarf.a' file under
`/usr/local/lib'. If you are in development phase, you put them
somewhere else, under a prefix different other than `/usr/local'.

   Now, how do we use libraries? Well, suppose that a program uses the
`barf' function defined in the barf library. Then a typical program
might look like:
     // -* main.c *-
     #include <stdio.h>
     #include <barf.h>
     main()
     {
      printf("This is barf!\n");
      barf();
      printf("Barf me!\n");
     }

If the library was installed in `/usr/local' then you can compile like
this:
     % gcc -c main.c
     % gcc main.o -o main -lbarf

Of course, if you did not install in `/prefix' instead of `/usr/local'
or `/usr' then you are in trouble. Now you have to do it this way:
     % gcc -I/prefix/include -c main.c
     % gcc main.o -o main -L/prefix/lib -lbarf

The `-I' flag tells the compiler where to find any extra header files
(like `barf.h') and the `-L' flag tells the compiler where to find any
extra libraries (like `libbarf.a'). The `-lbarf' flag tells the
compiler to bring in the entire `libbarf.a' library with all its
enclosed `.o' files and link it in with whathaveyou to produce the
executable.

   If the library hasn't been installed yet, and is present in the same
directory as the object file `main.o' then you can link them by passing
its filename instead:
     % gcc main.o libbarf.a -o main

Please link libraries with their full names if they haven't yet been
installed under the prefix directory and reserve using the `-l' flag
only for libraries that have already been installed. This is very
important. When you use Automake it helps it keep the dependencies
straight.  And when you use shared libraries, it is absolutely
essential.

   Also, please pay attention to the order with which you link your
libraries.  When the linker links a library, it does not embed into the
executable code the entire library, but only the symbols that are
needed from the library.  In order for the linker to know what symbols
are really needed from any given library, it must have already parsed
all the other libraries and object files that depend on that library!
This implies that you first link your object files, then you link the
higher-level libraries, then the lower-level libraries. If you are the
author of the libraries, you must write your libraries in such a
manner, that the dependency graph of your libraries is a tree. If two
libraries depend on each other bidirectionally, then you may have
trouble linking them in. This suggests that they should be one library
instead!

   While we are at the topic, when you compile ordinary programs like
the hello world program what really goes on behind the scenes is this:
     % gcc -c hello.c
     % gcc -o hello hello.o -lc

This links in the C system library `libc.a'.  The standard include files
that you use, such as `stdio.h', `stdlib.h' and whathaveyou are all
refering to various parts of these libraries. These libraries get
linked in by default when the `-o' flag is present. Note that other C
compilers may be calling their system libraries something else.  For
this reason the corresponding flags are assumed and you don't have to
supply them.

   The catch is that there are many functions that you think of as
standard that are not included in the `libc.a' library. For example all
the math functions that are declared in `math.h' are defined in a
library called `libm.a' which is not linked by default. So if the hello
world program needed the math library you should be doing this instead:
     % gcc -c hello.c
     % gcc -o hello hello.o -lm

On some old Linux systems it used to be required that you also link a
`libieee.a' library:
     % gcc -o hello hello.o -lieee -lm

More problems of this sort occur when you use more esoteric system
calls like sockets. Some systems require you to link in additional
system libraries such as `libbsd.a', `libsocket.a', `libnsl.a'. Also if
you are linking Fortran and C code together you must also link the
Fortran run-time libraries. These libraries have non-standard names and
depend on the Fortran compiler you use.  Finally, a very common problem
is encountered when you are writing X applications. The X libraries and
header files like to be placed in non-standard locations so you must
provide system-dependent `-I' and `-L' flags so that the compiler can
find them. Also the most recent version of X requires you to link in
some additional libraries on top of `libX11.a' and some rare systems
require you to link some additional system libraries to access
networking features (recall that X is built on top of the sockets
interface and it is essentially a communications protocol between the
computer running the program and computer that controls the screen in
which the X program is displayed.)  Fortunately, Autoconf can help you
deal with all of this. We will cover these issues in more detail in
subsequent chapters.

   Because it is necessary to link system libraries to form an
executable, under copyright law, the executable is derived work from
the system libraries.  This means that you must pay attention to the
license terms of these libraries.  The GNU `libc' library is under the
LGPL license which allows you to link and distribute both free and
proprietary executables. The `stdc++' library is also under terms that
permit the distribution of proprietary executables. The `libg++'
library however only permits you to build free executables. If you are
on a GNU system, including Linux-based GNU systems, the legalese is
pretty straightforward. If you are on a proprietary Unix system, you
need to be more careful. The GNU GPL does not allow GPLed code to be
linked against proprietary library. Because on Unix systems, the system
libraries are proprietary, their terms may not allow you to distribute
executables derived from them. In practice, they do however, since
proprietary Unix systems do want to attract proprietary applications.
In the same spirit, the GNU GPL also makes an exception and explicitly
permits the linking of GPL code with proprietary system libraries,
provided that said libraries _are_ system libraries. This includes
proprietary `libc.a' libraries, the `libdxml.a' library in Digital
Unix, proprietary Fortran system libraries like `libUfor.a', and the
X11 libraries.


File: tutorial.info,  Node: Using Automake and Autoconf,  Next: Using Autotools,  Prev: Compiling with Makefiles,  Up: Top

Using Automake and Autoconf
***************************

* Menu:

* Hello World revisited::
* OLD Using configuration headers::
* The building process::
* Some general advice::
* Standard organization with Automake::
* Programs and Libraries with Automake::
* General Automake principles::
* Simple Automake examples::
* Built sources::
* Installation directories.::
* Handling shell scripts::
* Handling other obscurities::


File: tutorial.info,  Node: Hello World revisited,  Next: OLD Using configuration headers,  Prev: Using Automake and Autoconf,  Up: Using Automake and Autoconf

Hello World revisited
=====================

   To begin, let's review the simplest example, the hello world program:
`hello.c'
          #include <stdio.h>
          main()
          {
           printf("Howdy, world!\n");
          }

`Makefile.am'
          bin_PROGRAMS = hello
          hello_SOURCES = hello.c

`configure.in'
          AC_INIT(hello.cc)
          AM_INIT_AUTOMAKE(hello,1.0)
          AC_PROG_CC
          AC_PROG_INSTALL
          AC_OUTPUT(Makefile)

   The language of `Makefile.am' is a "logic language". There is no
explicit statement of execution. Only a statement of relations from
which execution is inferred. On the other hand, the language of
`configure.in' is "procedural". Each line of `configure.in' is a
command that is executed.

   Seen in this light, here's what the `configure.in' commands shown do:
   * The `AC_INIT' command initializes the configure script. It must be
     passed as argument the name of one of the source files. Any source
     file will do.

   * The `AM_INIT_AUTOMAKE' performs some further initializations that
     are related to the fact that we are using `automake'. If you are
     writing your `Makefile.in' by hand, then you don't need to call
     this command.  The two comma-separated arguments are the name of
     the package and the version number.

   * The `AC_PROG_CC' checks to see which C compiler you have.

   * The `AC_PROG_INSTALL' checks to see whether your system has a BSD
     compatible install utility. If not then it uses `install-sh' which
     `automake' will install at the root of your package directory if
     it's not there yet.

   * The `AC_OUTPUT' tells the configure script to generate `Makefile'
     from `Makefile.in'

   The `Makefile.am' is more obvious. The first line specifies the name
of the program we are building. The second line specifies the source
files that compose the program.

   For now, as far as `configure.in' is concerned you need to know the
following additional facts:
   * If you are building a library, then your configure script must
     determine how to handle `ranlib'. To do that, add the
     `AC_PROG_RANLIB' command.

   * If you want to have your makefiles call recursively makefiles at
     subdirectories then the configure script needs to be told to find
     out how to do that.  For this purpose you add the
     `AC_PROG_MAKE_SET' command.

   * If you have any makefiles in subdirectories you must also put them
     in the `AC_OUTPUT' statement like this:
          AC_OUTPUT(Makefile          \
                    dir1/Makefile     \
                    dir2/Makefile     \
                   )

     Note that the backslashes are not needed if you are using the bash
     shell.  For portability reasons, however, it is a good idea to
     include them.

   As we explained before to build this package you need to execute the
following commands:
     % aclocal
     % autoconf
     % touch README AUTHORS NEWS ChangeLog
     % automake -a
     % configure
     % make

The first three commands, are for the maintainer only. When the user
unpacks a distribution, he should be able to start from `configure' and
move on.
   * The `aclocal' command installs a file called `aclocal.m4'.
     Normally, in that file you are supposed to place the definitions
     of any `autoconf' macros that you've written that happen to be in
     use in `configure.in'.  We will teach you how to write `autoconf'
     macros later.  The `automake' utility uses the `AM_INIT_AUTOMAKE'
     macro which is not part of the standard `autoconf' macros. For
     this reason, it's definition needs to be placed in `aclocal.m4'.
     If you call `aclocal' with no arguments then it will generate the
     appropriate `aclocal.m4' file.  Later we will show you how to use
     `aclocal' to also install your own `autoconf' macros.

   * The `autoconf' command combines the `aclocal.m4' and
     `configure.in' files and produces the `configure' script.  And now
     we are in bussiness.

   * The `touch' command makes the files `README' and friends exist.
     It is important that these files exist before calling Automake,
     because Automake decides whether to include them in a distribution
     by checking if they exist at the time that you invoke `automake'.
     Automake _must_ decide to include these files, because when you
     type `make distcheck' the presense of these files will be required.

   * The `automake' command compiles a `Makefile.in' file from
     `Makefile.am' and if absent it installs various files that are
     required either by the GNU coding standards or by the makefile
     that will be generated.

   If you are curious you can take a look at the generated `Makefile'.
It looks like gorilla spit but it will give you an idea of how one gets
there from the `Makefile.am'.

   The `configure' script is an information gatherer. It finds out
things about your system. That information is given to you in two ways.
One way is through defining C preprocessor macros that you can test for
directly in your source code with preprocessor directives. This is done
by passing `-D' flags to the compiler. The other way is by making
certain variables defined at the `Makefile.am' level. This way you can,
for example, have the configure script find out how a certain library
is linked, export is as a `Makefile.am' variable and use that variable
in your `Makefile.am'. Also, through certain special variables,
`configure' can control how the compiler is invoked by the `Makefile'.


File: tutorial.info,  Node: OLD Using configuration headers,  Next: The building process,  Prev: Hello World revisited,  Up: Using Automake and Autoconf

OLD Using configuration headers
===============================

   As you may have noticed, the `configure' script in the previous
example defines two preprocessor macros that you can use in your code:
`PACKAGE' and `VERSION'. As you become a power-user of `autoconf' you
will get define even more such macros. If you inspect the output of
`make' during compilation, you will see that these macros get defined
by passing `-D' flags to the compiler, one for each macro.  When there
is too many of these flags getting passed around, this can cause two
problems: it can make the `make' output hard to read, and more
importantly it can hit the buffer limits of various braindead
implementations of `make'. To work around this problem, an alternative
approach is to define all these macros in a special header file and
include it in all the sources.

   A hello world program using this technique looks like this
`configure.in'
          AC_INIT
          AM_CONFIG_HEADER(config.h)
          AM_INIT_AUTOMAKE(hello,0.1)
          AC_PROG_CXX
          AC_PROG_INSTALL
          AC_OUTPUT(Makefile)

`Makefile.am'
          bin_PROGRAMS = hello
          hello_SOURCES = hello.c

`hello.c'
          #ifdef HAVE_CONFIG_H
          #include <config.h>
          #endif
          
          #include <stdio.h>
          main()
          {
           printf("Howdy, pardner!\n");
          }

Note that we call a new macro in `configure.in': `AM_CONFIG_HEADER'.
Also we include the configuration file conditionally with the following
three lines:
     #ifdef HAVE_CONFIG_H
     #include <config.h>
     #endif

It is important to make sure that the `config.h' file is the first thing
that gets included. Now do the usual routine:
     % aclocal
     % autoconf
     % touch NEWS README AUTHORS ChangeLog
     % automake -a

Automake will give you an error message saying that it needs a file
called `config.h.in'. You can generate such a file with the `autoheader'
program. So run:
     % autoheader
     Symbol `PACKAGE' is not covered by acconfig.h
     Symbol `VERSION' is not covered by acconfig.h

Again, you get error messages. The problem is that `autoheader' is
bundled with the `autoconf' distribution, not the `automake'
distribution, and consequently doesn't know how to deal with the
`PACKAGE' and `VERSION' macros. Of course, if `configure' defines a
macro, there's nothing to know. On the other hand, when a macro _is not
defined_ then there are at least two possible defaults:
     #undef PACKAGE
     #define PACKAGE 0

The `autoheader' program here complains that it doesn't know the
defaults for the `PACKAGE' and `VERSION' macros. To provide the
defaults, create a new file `acconfig.h':
`acconfig.h'
          #undef PACKAGE
          #undef VERSION

and run `autoheader' again:
     % autoheader

At this point you must run `autoconf' again, so that it takes into
account the presense of `acconfig.h':
     % aclocal
     % autoconf

Now you can go ahead and build the program:
     % configure
     % make
     Computing dependencies for hello.cc...
     echo > .deps/.P
     gcc -DHAVE_CONFIG_H -I. -I. -I.   -g -O2 -c hello.cc
     gcc -g -O2  -o hello  hello.o

Note that now instead of multiple `-D' flags, there is only one such
flag passed: `-DHAVE_CONFIG_H'. Also, appropriate `-I' flags are passed
to make sure that `hello.cc' can find and include `config.h'.  To test
the distribution, type:
     % make distcheck
     ......
     ========================
     hello-0.1.tar.gz is ready for distribution
     ========================

and it should all work out.

   The `config.h' files go a long way back in history. In the past,
there used to be packages where you would have to manually edit
`config.h' files and adjust the macros you wanted defined by hand. This
made these packages very difficult to install because they required
intimate knowledge of your operating system. For example, it was not
unusual to see a comment saying _"if your system has a broken vfork,
then define this macro"_.  How the hell are you supposed to know if
your systems `vfork' is broken?? With auto-configuring packages all of
these details are taken care of automatically, shifting the burden from
the user to the developer where it belongs.

   Normally in the `acconfig.h' file you put statements like
     #undef MACRO
     #define MACRO default

These values are copied over to `config.h.in' and are supplemented with
additional defaults for C preprocessor macros that get defined by
native `autoconf' macros like `AC_CHECK_HEADERS', `AC_CHECK_FUNCS',
`AC_CHECK_SIZEOF', `AC_CHECK_LIB'.

   If the file `acconfig.h' contains the string `@TOP@' then all the
lines before the string will be included verbatim to `config.h'
_before_ the custom definitions. Also, if the file `acconfig.h'
contains the string `@BOTTOM@' then all the lines after the string will
be included verbatim to `config.h' _after_ the custom definitions.
This allows you to include further preprocessor directives that are
related to configuration. Some of these directives may be using the
custom definitions to conditionally issue further preprocessor
directives. Due to a bug in some versions of `autoheader' if the
strings `@TOP@' and `@BOTTOM@' do appear in your `acconfig.h' file,
then you must make sure that there is at least one line appearing before
`@TOP@' and one line after `@BOTTOM@', even if it has to be a comment.
Otherwise, `autoheader' may not work correctly.

   With `autotools' we distribute a utility called `acconfig' which
will build `acconfig.h' automatically. By default it will always make
sure that
     #undef PACKAGE
     #undef VERSION

are there. Additionally, if you install macros that are `acconfig'
friendly then `acconfig' will also install entries for these macros.
The `acconfig' program may be revised in the future and perhaps it
might be eliminated. There is an unofficial patch to Autoconf that will
automate the maintance of `acconfig.h', eliminating the need for a
seperate program. I am not yet certain if that patch will be part of
the official next version of Autoconf, but I very much expect it to.
Until then, if you are interested, see:
`http://www.clark.net/pub/dickey/autoconf/autoconf.html' This situation
creates a bit of a dilemma about whether I should document and
encourage `acconfig' in this tutorial or not.  I believe that the
Autoconf patch is a superior solution. However since I am not the one
maintaining Autoconf, my hands are tied. For now let's say that if you
confine yourself to using only the macros provided by `autoconf',
`automake', and `autotools' then `acconfig.h' will be completely taken
care for you by `acconfig'.  In the future, I hope that `acconfig.h'
will be generated by `configure' and be the sole responsibility of
Autoconf.

   You may be wondering whether it is worth using `config.h' files in
the programs you develop if there aren't all that many macros being
defined.  My personal recommendation is _yes_. Use `config.h' files
because perhaps in the future your `configure' might need to define
even more macros. So get started on the right foot from the beginning.
Also, it is nice to just have a `config.h' file lying around because
you can have all your configuration specific C preprocessor directives
in one place.  In fact, if you are one of these people writing peculiar
system software where you get to `#include' 20 header files on every
single source file you write, you can just have them on all thrown into
`config.h' once and for all.  In the next chapter we will tell you
about the `LF' macros that get distributed with `autotools' and this
tutorial. These macros do require you to use the `config.h' file. The
bottom line is: `config.h' is your friend; trust the `config.h'.


File: tutorial.info,  Node: The building process,  Next: Some general advice,  Prev: OLD Using configuration headers,  Up: Using Automake and Autoconf

The building process
====================

   FIXME: _write about VPATH builds and how to modify optimization_


File: tutorial.info,  Node: Some general advice,  Next: Standard organization with Automake,  Prev: The building process,  Up: Using Automake and Autoconf

Some general advice
===================

   In software engineering, people start from a precise, well-designed
specification and proceed to implementation. In research, the
specification is fluid and immaterial and the goal is to be able to
solve a slightly different problem every day. To have the flexibility
to go from variation to variation with the least amount of fuss is the
name of the game. By fuss, we refer to "debugging", "testing" and
"validation". Once you have a code that you know gives the right answer
to a specific set of problems, you want to be able to move on to a
different set of similar problems with reinventing, debugging and
testing as little as possible. These are the two distinct situations
that computer programmers get to confront in their lives.

   Software engineers can take good care of themselves in both
situations.  It's part of their training. However, people whose
specialty is the scientific problem and not software engineering, must
confront the hardest of the two cases, the second one, with very little
training in software engineering.  As a result they develop code that's
clumsy in implementation, clumsy in usage, and with only redeeming
quality the fact that it gives the right answer.  This way, they do get
the work of the day done, but they leave behind them no legacy to do
the work of tomorrow. No general-purpose tools, no documentation, no
reusable code.

   The key to better software engineering is to focus away from
developing monolithic applications that do only one job, and focus on
developing libraries. One way to think of libraries is as a _program
with multiple entry points_. Every library you write becomes a legacy
that you can pass on to other developers.  Just like in mathematics you
develop little theorems and use the little theorems to hide the
complexity in proving bigger theorems, in software engineering you
develop libraries to take care of low-level details once and for all so
that they are out of the way everytime you make a different
implementation for a variation of the problem.

   On a higher level you still don't create just one application. You
create many little applications that work together.  The centralized
all-in-one approach in my experience is far less flexible than the
decentralized approach in which a set of applications work together as
a team to accomplish the goal. In fact this is the fundamental principle
behind the design of the Unix operating system. Of course, it is still
important to glue together the various components to do the job. This
you can do either with scripting or with actually building a suite of
specialized monolithic applications derived from the underlying tools.

   The name of the game is like this: Break down the program to parts.
And the parts to smaller parts, until you get down to simple
subproblems that can be easily tested, and from which you can construct
variations of the original problem. Implement each one of these as a
library, write test code for each library and make sure that the
library works. It is very important for your library to have a complete
"test suite", a collection of programs that are supposed to run silently
and return normally (`exit(0);') if they execute successfully, and
return abnormally (`assert(false); exit(1);') if they fail.  The
purpose of the test suite is to detect bugs in the library, and to
convince you, the developer, that the library works. The best time to
write a test program is _as soon as it is possible!_ Don't be lazy.
Don't just keep throwing in code after code after code. The minute there
is enough new code in there to put together some kind of test program,
_just do it!_ I can not emphasize that enough. When you write new code
you have the illusion that you are producing work, only to find out
tomorrow that you need an entire week to debug it. As a rule,
internalize the reality that you _know_ you have produced new work
everytime you write a working test program for the new features, and
_not a minute before_.  Another time when you should definetly write a
test suite is when you find a bug while ordinarily using the library.
Then, before you even fix the bug, write a test program that detects
the bug. Then go fix it.  This way, as you add new features to your
libraries you have insurance that they won't reawaken old bugs.

   Please keep documentation up to date as you go. The best time to
write documentation is right after you get a few new test programs
working. You might feel that you are too busy to write documentation,
but the truth of the matter is that you will _always_ be too busy.
After long hours debugging these seg faults, think of it as a
celebration of triumph to fire up the editor and document your
brand-spanking new cool features.

   Please make sure that computational code is completely seperated
from I/O code so that someone else can reuse your computational code
without being forced to also follow your I/O model. Then write
_programs_ that invoke your collection of libraries to solve various
problems. By dividing and conquering the problem library by library
with a test suite for each step along the way, you can write good and
robust code. Also, if you are developing numerical software, please
don't expect that other users of your code will be getting a high while
entering data for your "input files". Instead write an interactive
utility that will allow users to configure input files in a user
friendly way. Granted, this is too much work in Fortran. Then again,
you do know more powerful languages, don't you?

   Examples of useful libraries are things like linear algebra
libraries, general ODE solvers, interpolation algorithms, and so on.
As a result you end up with two packages. A package of libraries
complete with a test suite, and a package of applications that invoke
the libraries.  The package of libraries is well-tested code that can
be passed down to future developers. It is code that won't have to be
rewritten if it's treated with respect. The package of applications is
something that each developer will probably rewrite since different
people will probably want to solve different problems. The effect of
having a package of libraries is that C++ is elevated to a _Very High
Level Language_ that's closer to the problems you are solving.  In fact
a good rule of thumb is to _make the libraries sufficiently
sophisticated so that each executable that you produce can be expressed
in *one* source file._ All this may sound like common sense, but you
will be surprised at how many scientific developers maintain just one
does-everything-program that they perpetually hack until it becomes
impossible to maintain. And then you will be even more surprised when
you find that some professors don't understand why a "simple
mathematical modification" of someone else's code is taking you so long.

   Every library must have its own directory and `Makefile'. So a
library package will have many subdirectories, each directory being one
library.  And perhaps if you have too many of them, you might want to
group them even further down. Then, there's the "applications". If
you've done everything right, there should be enough stuff in your
libraries to enable you to have one source file per application. Which
means that all the source files can probably go down under the same
directory.

   Very often you will come to a situation where there's something that
your libraries to-date can't do, so you implement it and stick it along
in your source file for the application. If you find yourself cut and
pasting that implementation to other source files, then this means that
you have to put this in a library somewhere. And if it doesn't belong
to any library you've written so far, maybe to a new library. When you
are in a deadline crunch, there's a tendency not to do this since it's
easier to cut and paste. The problem is that if you don't take action
right then, eventually your code will degenerate to a hard-to-use mess.
Keeping the entropy down is something that must be done on a daily
basis.

   Finally, a word about the age-old issue of language-choice. The GNU
coding standards encourage you to program in C and avoid using
languages other than C, such as C++ or Fortran. The main advantage of C
over C++ and Fortran is that it produces object files that can be
linked by any C or C++ compiler. In contrast, C++ object files can only
be linked by the compiler that produced them. As for Fortran, aside
from the fact that Fortran 90 and 95 have no free compilers, it is not
very trivial to mix Fortran 77 with C/C++, so it makes no sense to
invite all that trouble without a compelling reason. Nevertheless, my
suggestion is to code in C++.  The main benefit you get with C++ is
robustness. Having constructors and destructors and references can go a
long way towayrds helping you to void memory errors, if you know how to
make them work for you.


File: tutorial.info,  Node: Standard organization with Automake,  Next: Programs and Libraries with Automake,  Prev: Some general advice,  Up: Using Automake and Autoconf

Standard organization with Automake
===================================

   Now we get into the gory details of software organization. I'll tell
you one way to do it. This is advice, not divine will. It's simply a
way that works well in general, and a way that works well with
`autoconf' and `automake' in particular.

   The first principle is to maintain the package of libraries seperate
from the package of applications. This is not an iron-clad rule. In
software engineering, where you have a crystal clear specification, it
makes no sense to keep these two seperate. I found from experience that
it makes a lot more sense in research.  Either of these two packages
must have a toplevel directory under which live all of its guts. Now
what do the guts look like?

   First of all you have the traditional set of information files that
we described in Chapter 1:
     README, AUTHORS, NEWS, ChangeLog, INSTALL, COPYING

You also have the following subdirectories:
`m4'
     Here, you install any new `m4' files that your package may want to
     install. These files define new `autoconf' commands that you may
     want to make available to other developers who want to use your
     libraries.

`doc'
     Here you put the documentation for your code. You have the
     creative freedom to present the documentation in any way you
     desire. However, the prefered way to document software is to use
     Texinfo. Texinfo has the advantage that you can produce both
     on-line help as well as a nice printed book from the same source.
     We will say something about Texinfo later.

`src'
     Here's the source code. You could put it at the toplevel directory
     as many developers do, but I find it more convenient to keep it
     away in a subdirectory. Automake makes it trivially easy to do
     recursive `make', so there is no reason not to take advantage of
     it to keep your files more organized.

`include'
     This is an optional directory for distributions that use many
     libraries.  You can have the `configure' script link all public
     header files in all the subdirectories under `src' to this
     directory. This way it will only be necessary to pass one `-I'
     flag to test suites that want to access the include files of other
     libraries in the distribution.  We will discuss this later.

`lib'
     This is an optional directory where you put portability-related
     source code. This is mainly replacement implementations for system
     calls that may not exist on some systems. You can also put tools
     here that you commonly use accross many different packages, tools
     that are too simple to just make libraries out of every each one
     of them. It is suggested that you maintain these tools in a
     central place.  We will discuss this much later.  Together with
these subdirectories you need to put a `Makefile.am' and a
`configure.in' file. I also suggest that you put a shell script, which
you can call `reconf', that contains the following:
     #!/bin/sh
     rm -f config.cache
     rm -f acconfig.h
     touch acconfig.h
     aclocal -I m4
     autoconf
     autoheader
     acconfig
     automake -a
     exit

This will generate `configure' and `Makefile.in' and needs to be called
whenever you change a `Makefile.am' or a `configure.in' as well as when
you change something under the `m4' directory.  It will also call
`acconfig' which automatically generates `acconfig.h' and calle
`autoheader' to make `config.h.in'.  The `acconfig' utility is part of
`autotools', and if you are maintaining `acconfig.h' by hand, then you
want to use this script instead:
     #!/bin/sh
     rm -f config.cache
     aclocal -I m4
     autoconf
     autoheader
     automake -a
     exit

At the toplevel directory, you need to put a `Makefile.am' that will
tell the computer that all the source code is under the `src'
directory. The way to do it is to put the following lines in
`Makefile.am':
     EXTRA_DIST = reconf
     SUBDIRS = m4 doc src

   * The first line tells `automake' that the `reconf' script is part
     of the distribution and must be included when you do `make dist'.

   * The second line tells `automake' that the rest of the distribution
     is in the subdirectories `m4', `doc' and `src'. It instructs
     `make' to recursively call itself in these subdirectories. It is
     important to include the `doc' and `m4' subdirectories here and
     enhance them with `Makefile.am' so that `make dist' includes them
     into the distribution.

   If you are also using a `lib' subdirectory, then it should be built
before `src':
     EXTRA_DIST = reconf
     SUBDIRS = m4 doc lib src

The `lib' subdirectory should build a static library that is linked by
your executables in `src'. There should be no need to install that
library.

   At the toplevel directory you also need to put the `configure.in'
file. That should look like this:
     AC_INIT
     AM_INIT_AUTOMAKE(packagename,versionnumber)
     [...put your tests here...]
     AC_OUTPUT(Makefile                   \
               doc/Makefile               \
               m4/Makefile                \
               src/Makefile               \
               src/dir1/Makefile          \
               src/dir2/Makefile          \
               src/dir3/Makefile          \
               src/dir1/foo1/Makefile     \
               ............               \
              )

You will not need another `configure.in' file. However, *every
directory level on your tree must have a `Makefile.am'*.  When you call
`automake' on the top-level directory, it looks at `AC_OUTPUT' at your
`configure.in' to decide what other directories have a `Makefile.am'
that needs parsing. As you can see from above, a `Makefile.am' file is
needed even under the `doc' and `m4' directories. How to set that up is
up to you. If you aren't building anything, but just have files and
directories hanging around, you must declare these files and directories
in the `Makefile.am' like this:
     SUBDIRS = dir1 dir2 dir3
     EXTRA_DIST = file1 file2 file3

Doing that will cause `make dist' to include these files and directories
to the package distribution.

   This tedious setup work needs to be done everytime that you create a
new package. If you create enough packages to get sick of it, then you
want to look into the `acmkdir' utility that is distributed by
Autotools. We will describe it at the next chapter.


File: tutorial.info,  Node: Programs and Libraries with Automake,  Next: General Automake principles,  Prev: Standard organization with Automake,  Up: Using Automake and Autoconf

Programs and Libraries with Automake
====================================

   Next we explain how to develop `Makefile.am' files for the source
code directory levels. A `Makefile.am' is a set of assignments.  These
assignments imply the Makefile, a set of targets, dependencies and
rules, and the Makefile implies the execution of building.

   The first set of assignments going at the beginning look like this:
     INCLUDES = -I/dir1 -I/dir2 -I/dir3 ....
     LDFLAGS = -L/dir1 -L/dir2 -L/dir3 ....
     LDADD = -llib1 -llib2 -llib3 ...

   * The `INCLUDES' assignment is where you insert the `-I' flags that
     you need to pass to your compiler. If the stuff in this directory
     is dependent on a library in another directory of the same
     package, then the `-I' flag must point to that directory.

   * The `LDFLAGS' assignment is where you insert the `-L' flags that
     are needed by the compiler when it links all the object files to
     an executable.

   * The `LDADD' assignment is where you list a long set of installed
     libraries that you want to link in with all of your executables.
     Use the `-l' flag only for installed libraries. You can list
     libraries that have been built but not installed yet as well, but
     do this only be providing the full path to these libraries.

If your package contains subdirectories with libraries and you want to
link these libraries in another subdirectory you need to put `-I' and
`-L' flags in the two variables above. To express the path to these
other subdirectories, use the `$(top_srcdir)' variable.  For example if
you want to access a library under `src/libfoo' you can put something
like:
     INCLUDES = ... -I$(top_srcdir)/src/libfoo ...
     LDFLAGS  = ... -L$(top_srcdir)/src/libfoo ...

on the `Makefile.am' of every directory level that wants access to
these libraries. Also, you must make sure that the libraries are built
before the directory level is built. To guarantee that, list the library
directories in `SUBDIRS' *before* the directory levels that depend on
it. One way to do this is to put all the library directories under a
`lib' directory and all the executable directories under a `bin'
directory and on the `Makefile.am' for the directory level that
contains `lib' and `bin' list them as:
     SUBDIRS = lib bin

This will guarantee that all the libraries are available before building
any executables. Alternatively, you can simply order your directories
in such a way so that the library directories are built first.

   Next we list the things that are to be built in this directory level:
     bin_PROGRAMS    = prog1 prog2 prog3 ....
     lib_LIBRARIES   = libfoo1.a libfoo2.a libfoo3.a ....
     check_PROGRAMS  = test1 test2 test3 ....
     TESTS           = $(check_PROGRAMS)
     include_HEADERS = header1.h header2.h ....

   * The `bin_PROGRAMS' line lists all the executable files that will be
     compiled with `make' and installed with `make install' under
     `/prefix/bin', where `prefix' is usually `/usr/local'.

   * The `lib_LIBRARIES' line lists all the library files that will be
     compiled with `make' and installed with `make install' under
     `/prefix/lib'.

   * The `check_PROGRAMS' line lists executable files that are *not*
     compiled with a simple `make' but only with a `make check'. These
     programs serve as tests that you, the user can use to test the
     library.

   * The `TESTS' line lists executable files which are to be compiled
     _and executed_ when you run `make check'. These programs
     constitute the "test suite" and they are indispensible when you
     develop a library. It is common to just set
          TESTS = $(check_PROGRAMS)

     This way by commenting the line in and out, you can modify the
     behaviour of `make check'. While debugging your test suite, you
     will want to comment out this line so that `make check' doesn't
     run it. However, in the end product, you will want to comment it
     back in.

   * The `include_HEADERS' line lists public headers present in this
     directory that you want to install in `/prefix/include'. You must
     list a header file here if you want to cause it to be installed.
     You can also list it under `libfoo_a_SOURCES' for the library that
     it belongs to, but it is imperative to list public headers here so
     that they can be installed.

It is good programming practice to keep libraries and executables under
seperate directory levels. However, it is okey to keep the library and
the _check_ executables that test the library under the same directory
level because that makes it easier for you to link them with the
library.

   For each of these types of targets, we must state information that
will allow `automake' and `make' to infer the building process.

   * *For each Program:* You need to declare the set of files that are
     sources of the program, the set of libraries that must be linked
     with the program and (optionally) a set of dependencies that need
     to be built before the program is built. These are declared in
     assignments that look like this:
          prog1_SOURCES = foo1.cc foo2.cc ... header1.h header2.h ....
          prog1_LDADD   = -lbar1 -lbar2 -lbar3
          prog1_LDFLAGS = -L/dir1 -L/dir2 -L/dir3 ...
          prog1_DEPENDENCIES = dep1 dep2 dep3 ...

     In each assignment substitute `prog1' with the name of the program
     that you are building as it appeared in `bin_PROGRAMS' or
     `check_PROGRAMS'.
        - `prog1_SOURCES': Here you list all the `*.cc' and `*.h' files
          that compose the source code of the program. The presense of
          a header file here doesn't cause the file to be installed at
          `/prefix/include' but it does cause it to be added to the
          distribution when you do `make dist'.  To cause header files
          to be installed you must also put them in `include_HEADERS'.

        - `prog1_LDADD': Here you add primarily the `-l' flags for
          linking whatever libraries are needed by your code. You may
          also list object files, which have been compiled in an exotic
          way, as well as paths to uninstalled yet libraries.

        - `prog_LDFLAGS': Here you add the `-L' flags that are needed to
          resolve the libraries you passed in `prog_LDADD'. Certain
          flags that need to be passed on _every_ program can be
          expressed on a global basis by assigning them at `LDFLAGS'.

        - `prog1_DEPENDENCIES': If for any reason you want certain
          other targets to be built before building this program, you
          can list them here.
     This is all you need to do. There is no need to write an extended
     Makefile with all the targets, dependencies and rules that are
     required to build the program. They are computed for you by this
     minimal information by `automake'. Moreover, the targets `dist',
     `install', `clean' and `distclean' are appropriately setup to
     handle the program. You don't need to take care of them by
     yourself.

   * *For each Library:* There's a total of four assignments that are
     relevant to building libraries:
          lib_LIBRARIES = ... libfoo1.a ...
          libfoo1_a_SOURCES      = foo1.cc foo2.cc private1.h private2.h ...
          libfoo1_a_LIBADD       = obj1.o obj2.o obj3.o
          libfoo1_a_DEPENDENCIES = dep1 dep2 dep3 ...

     Note that if the name of the library is `libfoo1.a' the prefix that
     appears in the variables that are related with that library is
     `libfoo1_a_'.
        - `libfoo1_a_SOURCES': Just like with programs, here you list
          all the `*.cc' files as well as all the *private* header
          files that compose the library. By "private header file" we
          mean a header file that is used internally by the library and
          the maintainers of the library, but is not exported to the
          end-user. You can list "public header files" also if you
          like, and perhaps you should for documentation purposes, but
          if you mention them in `include_HEADERS' it is not required
          to repeat them a second time here.

        - `libfoo1_a_DEPENDENCIES': If there are any other targets that
          need to be built before this library is built, list them here.

        - `libfoo1_a_LIBADD': If there are any other object files that
          you want to include in the library list them here. You might
          be tempted to list them as dependencies in
          `libfoo1_a_DEPENDENCIES', but that will not work. If you do
          that, the object files will be built before the library is
          built _but they will not be included in the library!_ By
          listing an object file here, you are stating that you want it
          to be built *and* you want it to be included in the library.

